#!/usr/bin/env python3
"""
Script d'export du mod√®le MNIST PyTorch vers ONNX
"""

import os
import argparse
import torch
import torch.onnx
import onnx
import onnxruntime as ort
import numpy as np

from src.model import MNISTModel, MNISTNet
from src.data_loader import MNISTDataLoader

def export_to_onnx(model_path, onnx_path, batch_size=1):
    """Exporte le mod√®le PyTorch vers ONNX"""
    
    print("üîÑ D√©but de l'export ONNX...")
    
    # Charger le mod√®le PyTorch
    print(f"üìÇ Chargement du mod√®le depuis {model_path}")
    device = torch.device('cpu')  # ONNX fonctionne mieux avec CPU
    model = MNISTModel(device=device)
    model.load_model(model_path)
    model.model.eval()
    
    # Cr√©er un exemple d'entr√©e
    dummy_input = torch.randn(batch_size, 1, 28, 28, device=device)
    
    print(f"Input shape: {dummy_input.shape}")
    
    # Export vers ONNX
    print(f"Export vers {onnx_path}...")
    
    torch.onnx.export(
        model.model,                     # Mod√®le PyTorch
        dummy_input,                     # Exemple d'entr√©e
        onnx_path,                       # Chemin de sortie
        export_params=True,              # Exporter les param√®tres
        opset_version=17,                # Version ONNX (compatible avec onnxruntime-web)
        do_constant_folding=True,        # Optimisations
        input_names=['input'],           # Noms des entr√©es
        output_names=['output'],         # Noms des sorties
        dynamic_axes={                   # Axes dynamiques
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    
    print("‚úÖ Export ONNX termin√©!")
    
    # V√©rification du mod√®le ONNX
    print("üîç V√©rification du mod√®le ONNX...")
    try:
        onnx_model = onnx.load(onnx_path)
        onnx.checker.check_model(onnx_model)
        print("‚úÖ Mod√®le ONNX valide!")
        
        # Afficher les informations du mod√®le
        print(f"\nüìä Informations du mod√®le ONNX:")
        print(f"   - Version ONNX: {onnx_model.opset_import[0].version}")
        print(f"   - Inputs: {[input.name for input in onnx_model.graph.input]}")
        print(f"   - Outputs: {[output.name for output in onnx_model.graph.output]}")
        
        # Taille du fichier
        file_size = os.path.getsize(onnx_path) / (1024 * 1024)
        print(f"   - Taille du fichier: {file_size:.2f} MB")
        
    except Exception as e:
        print(f"‚ùå Erreur lors de la v√©rification: {e}")
        return False
    
    return True

def test_onnx_model(onnx_path, pytorch_model_path, num_tests=5):
    """Teste le mod√®le ONNX et compare avec PyTorch"""
    
    print(f"\nüß™ Test du mod√®le ONNX avec {num_tests} √©chantillons...")
    
    # Charger le mod√®le PyTorch original
    device = torch.device('cpu')
    pytorch_model = MNISTModel(device=device)
    pytorch_model.load_model(pytorch_model_path)
    pytorch_model.model.eval()
    
    # Charger le mod√®le ONNX
    ort_session = ort.InferenceSession(onnx_path)
    
    # Charger quelques donn√©es de test
    data_loader = MNISTDataLoader(batch_size=1)
    _, _, test_loader = data_loader.load_data()
    
    print("\nüìä Comparaison PyTorch vs ONNX:")
    print("-" * 60)
    
    correct_matches = 0
    
    for i, (data, target) in enumerate(test_loader):
        if i >= num_tests:
            break
        
        # Pr√©diction PyTorch
        with torch.no_grad():
            pytorch_output = pytorch_model.model(data)
            pytorch_pred = torch.argmax(pytorch_output, dim=1).item()
            pytorch_conf = torch.softmax(pytorch_output, dim=1).max().item()
        
        # Pr√©diction ONNX
        ort_inputs = {ort_session.get_inputs()[0].name: data.numpy()}
        ort_output = ort_session.run(None, ort_inputs)[0]
        
        # Convertir log_softmax en probabilities
        onnx_probs = np.exp(ort_output)
        onnx_pred = np.argmax(onnx_probs, axis=1)[0]
        onnx_conf = np.max(onnx_probs)
        
        # V√©rifier la correspondance
        match = "‚úÖ" if pytorch_pred == onnx_pred else "‚ùå"
        if pytorch_pred == onnx_pred:
            correct_matches += 1
        
        true_label = target.item()
        
        print(f"√âchantillon {i+1}: Vrai={true_label} | "
              f"PyTorch={pytorch_pred}({pytorch_conf:.3f}) | "
              f"ONNX={onnx_pred}({onnx_conf:.3f}) {match}")
    
    accuracy = correct_matches / num_tests * 100
    print("-" * 60)
    print(f"Correspondance PyTorch-ONNX: {correct_matches}/{num_tests} ({accuracy:.1f}%)")
    
    if accuracy == 100:
        print("Parfait! Le mod√®le ONNX est identique au mod√®le PyTorch!")
    elif accuracy >= 95:
        print("üëç Tr√®s bon! Diff√©rences mineures acceptables.")
    else:
        print("Attention: Diff√©rences significatives d√©tect√©es.")
    
    return accuracy

def optimize_onnx_model(onnx_path, optimized_path):
    """Optimise le mod√®le ONNX pour le web"""
    
    print(f"\nOptimisation du mod√®le ONNX...")
    
    try:
        import onnxoptimizer
        
        # Charger le mod√®le
        model = onnx.load(onnx_path)
        
        # Appliquer les optimisations
        optimized_model = onnxoptimizer.optimize(model)
        
        # Sauvegarder le mod√®le optimis√©
        onnx.save(optimized_model, optimized_path)
        
        # Comparer les tailles
        original_size = os.path.getsize(onnx_path) / (1024 * 1024)
        optimized_size = os.path.getsize(optimized_path) / (1024 * 1024)
        reduction = (1 - optimized_size/original_size) * 100
        
        print(f"üìä R√©sultats de l'optimisation:")
        print(f"   - Taille originale: {original_size:.2f} MB")
        print(f"   - Taille optimis√©e: {optimized_size:.2f} MB")
        print(f"   - R√©duction: {reduction:.1f}%")
        
        return True
        
    except ImportError:
        print("onnxoptimizer non install√©. Installation...")
        os.system("pip install onnxoptimizer")
        return False
    except Exception as e:
        print(f"‚ùå Erreur lors de l'optimisation: {e}")
        return False

def create_web_assets(onnx_path):
    """Cr√©e les assets pour l'utilisation web"""
    
    print(f"\nüåê Cr√©ation des assets web...")
    
    # Cr√©er le dossier static si il n'existe pas
    os.makedirs('static/models', exist_ok=True)
    
    # Copier le mod√®le ONNX dans static
    import shutil
    web_model_path = 'static/models/mnist_model.onnx'
    shutil.copy2(onnx_path, web_model_path)
    
    # Cr√©er un fichier de m√©tadonn√©es JSON
    import json
    
    metadata = {
        "model_name": "MNIST CNN",
        "framework": "PyTorch -> ONNX",
        "input_shape": [1, 1, 28, 28],
        "output_shape": [1, 10],
        "input_name": "input",
        "output_name": "output",
        "classes": list(range(10)),
        "preprocessing": {
            "normalize": True,
            "mean": 0.1307,
            "std": 0.3081
        }
    }
    
    with open('static/models/model_metadata.json', 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"‚úÖ Assets web cr√©√©s:")
    print(f"   - Mod√®le: {web_model_path}")
    print(f"   - M√©tadonn√©es: static/models/model_metadata.json")

def main():
    parser = argparse.ArgumentParser(description='Exporter le mod√®le MNIST vers ONNX')
    parser.add_argument('--model_path', type=str, default='models/mnist_model.pth',
                       help='Chemin du mod√®le PyTorch (d√©faut: models/mnist_model.pth)')
    parser.add_argument('--onnx_path', type=str, default='models/mnist_model.onnx',
                       help='Chemin de sortie ONNX (d√©faut: models/mnist_model.onnx)')
    parser.add_argument('--batch_size', type=int, default=1,
                       help='Taille du batch pour l\'export (d√©faut: 1)')
    parser.add_argument('--test', action='store_true',
                       help='Tester le mod√®le ONNX apr√®s export')
    parser.add_argument('--optimize', action='store_true',
                       help='Optimiser le mod√®le ONNX')
    parser.add_argument('--web', action='store_true',
                       help='Cr√©er les assets pour le web')
    
    args = parser.parse_args()
    
    print("=== üîÑ Export ONNX du Mod√®le MNIST ===\n")
    
    # V√©rifier que le mod√®le PyTorch existe
    if not os.path.exists(args.model_path):
        print(f"‚ùå Erreur: Le mod√®le {args.model_path} n'existe pas.")
        print("Veuillez d'abord entra√Æner le mod√®le avec: python train.py")
        return
    
    # Cr√©er le dossier models si n√©cessaire
    os.makedirs('models', exist_ok=True)
    
    # Export vers ONNX
    success = export_to_onnx(args.model_path, args.onnx_path, args.batch_size)
    
    if not success:
        print("‚ùå L'export ONNX a √©chou√©.")
        return
    
    # Test du mod√®le ONNX
    if args.test:
        test_onnx_model(args.onnx_path, args.model_path)
    
    # Optimisation
    if args.optimize:
        optimized_path = args.onnx_path.replace('.onnx', '_optimized.onnx')
        optimize_onnx_model(args.onnx_path, optimized_path)
    
    # Cr√©ation des assets web
    if args.web:
        create_web_assets(args.onnx_path)
    
    print(f"\nExport ONNX termin√© avec succ√®s!")
    print(f"üìÅ Mod√®le ONNX: {args.onnx_path}")
    
    # Instructions pour l'utilisation
    print(f"\nUtilisation:")
    print(f"   - Web: python export_onnx.py --web")
    print(f"   - Test: python export_onnx.py --test")
    print(f"   - Optimisation: python export_onnx.py --optimize")

if __name__ == "__main__":
    main() 